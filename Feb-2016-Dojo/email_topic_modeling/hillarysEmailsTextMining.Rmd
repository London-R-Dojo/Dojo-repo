---
title: "Text Mining Hillary Clinton’s Emails"
author: "Fabio Amaral"
date: "14 February 2016"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(stringsAsFactors = FALSE)
```

## Dependencies

This document depends on the following packages:
```{r load  packages}
# Load packages
suppressPackageStartupMessages(library("BBmisc"))

pkgs <- c("stringi", "stringr", "tm", "wordcloud", "lda", "LDAvis", 
          "stm", "stmBrowser", "lubridate", "qdap", "png", "grid")

suppressAll(lib(pkgs)); rm(pkgs)
```

## Data Import 

The Hillary Clinton's Email dataset is publicly available and was downladed from [Kaggle website]("https://www.kaggle.com/kaggle/hillary-clinton-emails"). For this link to work one must be logged in to a [Kaggle user account]("https://www.kaggle.com/account/login?returnUrl=%2Faccount%2Fregister").

```{r load dataset}
dat <- read.csv("dataset/Emails.csv",
                sep = ",",
                encoding = "UTF-8", 
                header = T, 
                stringsAsFactors = F)
```

The description of the datset variables can be found in the Kaggle main page for the [Hillary Clinton's Email dataset.](https://www.kaggle.com/kaggle/hillary-clinton-emails) 

```{r dataset dimensions}
dim(dat)
```

```{r variables}
names(dat)
```

## Data Cleaning

Before anything we must tidy up the data a little bit.
```{r encoding}
# Check encoding of text 
table(Encoding(dat$ExtractedBodyText))
```

The dataset encoding is not fully recognised which can cause some problems. 
I will first try to address this issue.

```{r stri_enc_mark}
text <- dat$ExtractedBodyText
table(stri_enc_mark(text))
```
The **stringi** ``stri_enc_mark`` function is able to recognise the uknown encoding as **ASCII**. I will now try to standardize the encoding.

```{r Enforce UTF-8, warning=FALSE}
# Enforce UTF-8 (Not working)
text <- stri_encode(text, "ASCII", "UTF-8")
table(Encoding(text))

text <- enc2native(text)
table(Encoding(text))
```

The encoding conversion did not work so it will be left as is for the time being.

It will be interesting to identify which emails were sent to or from Hillary Clinton. So now we focus on organising the sender informartion.
```{r sender}
# Identify emails from Hillary
head(dat[, c("MetadataFrom", "SenderPersonId")]) # SenderPersonId == 80

# 157 emails witout sender information
sum(dat$MetadataFrom == "") 

# Have a look at the emails with empty sender
# dat$ExtractedBodyText[dat$MetadataFrom == ""] # Lots of empty text as well
```

Some emails we can deduce it's from or to Hillary but I'll remove them since they are not so many.

```{r remove blank sender}
# Remove cases with MetadataFrom == ""
dat <- dat[dat$MetadataFrom != "",]
# dim(dat)
```

There are over 100 emails without text.
```{r empty emails}
sum(dat$ExtractedBodyText == "") # 1203 emails have no body text
```

I'll have a look at the raw text emails to see if I can retrive some emails that have not been previously extracted.
```{r retrive some emails from raw text}
# Look at RawText to see if some more emails can be extracted
dat$RawText[dat$ExtractedBodyText == ""][1]
```

Many emails have text between "Subject:" and "UNCLASSIFIED"
```{r extract text}
# Extract text
emptyBodyText_Idx <- dat$ExtractedBodyText == ""

# replace all line breaks from RawText with a space
dat$RawText <- gsub("\n", " ", dat$RawText )

# Copy every thing between "Subject: and "UNCLASSIFIED""
text_field <- "Subject\\: (.*)UNCLASSIFIED"

# 839 extra texts can be retrived
sum(str_detect(dat$RawText[emptyBodyText_Idx], text_field)) 

# retrive text
retrived_text <- str_extract(dat$RawText[emptyBodyText_Idx], text_field)
```

```{r clean text}
# Clean it up
NAidx <- is.na(retrived_text)
retrived_text[NAidx] <- ""
subject <- "Subject\\:(.*)Subject\\:"
retrived_text <- gsub(subject, "", retrived_text)
retrived_text <- gsub("UNCLASSIFIED", "", retrived_text)
head(retrived_text, 1)
```

Enter recovered text to the empty **ExtractedBodyText** variable.
```{r retrived text}
# Add retrived text
dat$ExtractedBodyText[emptyBodyText_Idx] <- retrived_text

# Remove emails still without body text
empty_idx <- dat$ExtractedBodyText == ""
sum(empty_idx) # 364
dat <- dat[!empty_idx, ]
dim(dat) # 7581   22
```

Dates will be interesting covariates in our analysis, so we will now focus on cleaning their data.
```{r dates}
# Clean up date variable
date <- dat$MetadataDateSent
date <- gsub("T(.*)", "", date)
date <- ymd(date)

sum(is.na(date)) # created 3 NAs
na_idx <- which(is.na(date))
dat$MetadataDateSent[na_idx] # was empty in MetadataDateSent
dat$ExtractedDateSent[na_idx] # was empty in ExtractedDateSent as well
```

Input the few missing dates
```{r inputation}
# Check the dates before and after the NAs to see if I can use for inputation

# Day after 
dat$MetadataDateSent[na_idx+1]

# Day before
dat$MetadataDateSent[na_idx-1]

# Input date with the next day info
date[na_idx] <- date[na_idx+1]
sum(is.na(date))
```

Create variables for days, months and year
```{r }
day_month <- day(date)
day_week <- wday(date, label = TRUE, abbr = FALSE)
month <- month(date, label = TRUE, abbr = FALSE)
year <- year(date)
# The emails are from 2009 to 2014
table(year)
```

```{r new data.frame}
# Make new dataframe to store processed data
emails <- data.frame(to = dat$ExtractedTo,
                     from = dat$ExtractedFrom,
                     senderId = dat$SenderPersonId,
                     released = dat$ExtractedReleaseInPartOrFull,
                     emails = dat$ExtractedBodyText,
                     text = dat$ExtractedBodyText,
                     rawtext = dat$RawText,
                     date = date,
                     day_month = day_month,
                     day_week = day_week,
                     month = month,
                     year = year)

# Clean up environment
rm(list = ls()[-5]) #keep only emails data.frame
dim(emails)
names(emails)
```

Create a binary variable ``from_to`` to denote if the email was sent or recived by Hillary.
```{r from_to variable}
# Classify emails as to and from Hillary
emails$from_to <- as.factor(ifelse(emails$senderId == 80, "From Hillary", "To Hillary"))

table(emails$from_to)
```

```{r}
# Quick look at some emails text
head(emails$text, 4)
```

The text needs some cleaning
```{r clean up text}
text <- emails$text

# Remove email addresses
email_address <- "<(.*)>"
text <- gsub(email_address, " ", text)
text <- gsub("(.*)\\.com", " ", text)
text <- gsub("^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}$", "", text)
text <- gsub("(.*)@state\\.gov", "", text)

# Remove line break
text <- gsub("\\n", " ", text)

# Remove dates
week_days <- "(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)"
months <- "(January|February|March|April|May|June|July|August|September|October|November|December)"

# Remove dates
text <- gsub(week_days, " ", text)
text <- gsub(months, " ", text)

# More tidy up
text <- gsub("[A-P]M", " ", text)
text <- gsub("B\\d", " ", text)
text <- gsub("\"", " ", text)
text <- gsub("[T-t]o\\:|[F-f]rom\\:|H\\:|[F-f]or\\:|[S-s]ent\\:|[R-r][E-e]\\:|FW\\:|Fw\\:|Fwd\\:|mailto\\:|Tel\\:", " ", text)
text <- gsub("Subject\\:", "", text)
text <- gsub("\\/(.*)\\/", "", text)
text <- gsub("^http\\:(.*)", "", text)

# head(text, 50)
# tail(text, 50)
# text[1000:1050]
```

```{r more processing}
# Correct some words
# Pis
text <- gsub("Pis", "Pls", text)

# More pre-processing:
text <- gsub("'", "", text)  # remove apostrophes
text <- gsub("•", "", text) # remove •
text <- gsub("[[:punct:]]", "", text)  # remove punctuation 
text <- gsub("[[:cntrl:]]", " ", text)  # replace control characters with space
text <- gsub("^[[:space:]]+", "", text) # remove whitespace at beginning of documents
text <- gsub("[[:space:]]+$", "", text) # remove whitespace at end of documents
text <- gsub("[[:digit:]]", "", text) # remove numbers
text <- tolower(text)  # force to lowercase
text <- gsub("h ", "", text)
text <- gsub("w ", "", text)
text <- gsub("pm ", "", text)
text <- gsub("imagejpg", "", text)

emails$text <- text

# remove empty emails
empty <- emails$text == "" | emails$text == " "
emails <- emails[!empty, ]

dim(emails)
```

## N-gram Frequencies

```{r ngrams}
txt <- emails$text
txt <- removeWords(txt, words = stopwords("english"))

txt_TO <- txt[emails$from_to == "To Hillary"]
length(txt_TO)

txt_FROM <- txt[emails$from_to == "From Hillary"]
length(txt_FROM)

out.1 <- tau::textcnt(x = txt, 
                      method = "string", 
                      n = 1, 
                      decreasing = TRUE)

out.1_TO <- tau::textcnt(x = txt_TO, 
                      method = "string", 
                      n = 1, 
                      decreasing = TRUE)

out.1_FROM <- tau::textcnt(x = txt_FROM, 
                         method = "string", 
                         n = 1, 
                         decreasing = TRUE)

# Frequencies of bigrams
out.2 <- tau::textcnt(x = txt, 
                    method = "string", 
                    n = 2, 
                    decreasing = TRUE)

out.2_TO <- tau::textcnt(x = txt_TO, 
                      method = "string", 
                      n = 2, 
                      decreasing = TRUE)

out.2_FROM <- tau::textcnt(x = txt_FROM, 
                      method = "string", 
                      n = 2, 
                      decreasing = TRUE)

# Frequencies of trigrams
out.3 <- tau::textcnt(x = txt, 
                    method = "string", 
                    n = 3, 
                    decreasing = TRUE)

out.3_TO <- tau::textcnt(x = txt_TO, 
                      method = "string", 
                      n = 3, 
                      decreasing = TRUE)

out.3_FROM <- tau::textcnt(x = txt_FROM, 
                      method = "string", 
                      n = 3, 
                      decreasing = TRUE)
```

```{r one word frequencies, fig.height=10, fig.width=14}
par(mfrow=c(2,3), mar=c(2,6,2,2))
# One words barplot
barplot(rev(head(out.1, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of terms\n(Combined)")

barplot(rev(head(out.1_TO, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of terms\n(To Hillary)")

barplot(rev(head(out.1_FROM, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of terms\n(From Hillary)")

# One words wordcloud
wordcloud(names(out.1), freq = out.1, scale = c(5, .05), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))


wordcloud(names(out.1_TO), freq = out.1, scale = c(5, .05), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))

wordcloud(names(out.1_FROM), freq = out.1, scale = c(5, .05), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))
```

```{r bi-gram frequencies, fig.height=10, fig.width=14}
par(mfrow=c(2,3), mar=c(2,8,2,3))
# Bigrams barplot
barplot(rev(head(out.2, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of bi-grams\n(Combined)")

barplot(rev(head(out.2_TO, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of bi-grams\n(To Hillary)")

barplot(rev(head(out.2_FROM, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of bi-grams\n(From Hillary)")

# Bigrams wordcloud
wordcloud(names(out.2), freq = out.2, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))

wordcloud(names(out.2_TO), freq = out.2, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))

wordcloud(names(out.2_FROM), freq = out.2, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))
```

```{r tri-gram frequencies, fig.height=10, fig.width=14}
par(mfrow=c(2,3), mar=c(2,12,2,3))
# Trigrams barplot
barplot(rev(head(out.3, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of tri-grams\n(Combined)")

barplot(rev(head(out.3_TO, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of tri-grams\n(To Hillary)")

barplot(rev(head(out.3_FROM, 20)), col ="orange",
        horiz = TRUE, las=1, main = "Frequency of tri-grams\n(From Hillary)")

# Trigrams wordcloud
wordcloud(names(out.3), freq = out.3, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))

wordcloud(names(out.3_TO), freq = out.3, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))

wordcloud(names(out.3_FROM), freq = out.3, scale = c(2, .0005), min.freq = 10,
          max.words = 150, random.order = FALSE, colors = brewer.pal(6,"Dark2"))
```

## Topic Modeling

### Latent Dirichlet Alocation (LDA)

```{r LDA data preparation}
# example @ http://cpsievert.github.io/LDAvis/reviews/reviews.html

# tokenize on space and output as a list:
doc.list <- strsplit(txt, "[[:space:]]+")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# read in some stopwords:
stop_words <- stopwords("SMART")

# remove terms that are stop words or occur fewer than 3 times or are "":
del <- names(term.table) %in% stop_words | term.table < 3 
term.table <- term.table[!del]

# Remove empty string term
head(names(term.table))
empty <- names(term.table) == ""
term.table <- term.table[!empty]

vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
        index <- match(x, vocab)
        index <- index[!is.na(index)]
        rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
        }

documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)  # number of documents (7393)
W <- length(vocab)  # number of terms in the vocab (10296)
# number of tokens per document [7, 1, 14, 15, 118, ...]
doc.length <- sapply(documents, function(x) sum(x[2, ])) 

# total number of tokens in the data (198088)
N <- sum(doc.length)  

# frequencies of terms in the corpus [1114, 984, 837, 756, 666, ...]
term.frequency <- as.integer(term.table)  
```

```{r fit model, cache=TRUE}
# MCMC and model tuning parameters:
K <- 30 # no.topics
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  # 12.7376 mins
# beepr::beep(0)
```

```{r LDAvis, eval=FALSE}
# Estimate document-topic distributions(D * K matrix θ)
# Set topic-term distributions (K * W matrix ϕ)

theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))

emails4LDA<- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)

saveRDS(emails4LDA, file = "emails4LDA.RDS")
```

```{r eval=FALSE}
emails4LDA <- readRDS("emails4LDA.RDS")

# create the JSON object to feed the visualization:
json <- createJSON(phi = emails4LDA$phi, 
                   theta = emails4LDA$theta, 
                   doc.length = emails4LDA$doc.length, 
                   vocab = emails4LDA$vocab, 
                   term.frequency = emails4LDA$term.frequency)

serVis(json, out.dir = 'LDAvis', open.browser = TRUE)
```

[
```{r, lda image, fig.width=8,echo=FALSE, cache=TRUE}
img <- readPNG("images/lda.png")
grid.raster(img)
```
](http://fredus14.github.io/Hillary/#topic=14&lambda=1&term=)

Visualization of the LDA analysis created with the LDAvis package. Click on this [link](http://fredus14.github.io/Hillary/#topic=14&lambda=1&term=) or on the figure above to open the analysis in a web browser for interactive visualization.

### Structural Topic Model

```{r prepare data for stm}
#stemming/stopword removal, etc.
emails$from_to <- factor(emails$from_to)
emails$released <- factor(emails$released)
emails$day_month <- factor(emails$day_month)
emails$year <- factor(emails$year)

stm_data <- emails[,c("released", "senderId", "to", "from",
                      "from_to", "emails","text", "rawtext", "date", 
                      "day_month", "day_week", "month", "year")]

month_names <- tolower(as.character(unique(emails$month)))
week_days_names <- tolower(as.character(unique(emails$day_week)))
```

### Sentiment as Topic Model Covariate

```{r sentiment analysis, cache=TRUE, warning=FALSE, fig.height=6}
# Compute the sentiment score on a [-1,1] range
txt <- tolower(stm_data$text)
t1 <- Sys.time()
sentiments <- polarity(txt,
                       polarity.frame = qdapDictionaries::key.pol,
                       negators = qdapDictionaries::negation.words,
                       amplifiers = qdapDictionaries::amplification.words,
                       deamplifiers = qdapDictionaries::deamplification.words, 
                       amplifier.weight = 0.8,
                       n.before = 4, 
                       n.after = 2,
                       constrain = TRUE)
t2 <- Sys.time()
t2-t1 # 2.439788 mins
# beepr::beep(0)

# Add a column called sentiment to stm_data data.frame 
stm_data$sentiment <- sentiments$all$polarity

# General output
sentiments$group

## Structure of the detailed output
# str(sentiments$all)

# Distribution of the sentiment - standard a lot of zero's
stm_data[which(is.na(stm_data$sentiment)),] # one email have no text -> leads to NaN sentiment
summary(stm_data$sentiment)

par(mfrow=c(1,2))
hist(stm_data$sentiment, 
     col = "orange", 
     xlab = "sentiment [-1, 1]", 
     main = "")

boxplot(stm_data$sentiment, 
        range = F, 
        col = "orange",
        ylab = "sentiment [-1, 1]")
```

```{r process dataset for lda}
# Remove case with sentiment as NA (5801)
idx <- which(is.na(stm_data$sentiment))
stm_data <- stm_data[-idx,]

stop_words <- c(stop_words, "imagejpg", "Subject:", "AM", "PM", month_names, week_days_names)

processed <- textProcessor(stm_data$text, 
                           metadata=stm_data, 
                           stem = FALSE,
                           striphtml = TRUE,
                           customstopwords = stop_words,
                           verbose=FALSE)
```

```{r frequency threshold, fig.height=5, fig.width=14}
# Choose frequency threshold
plotRemoved(processed$documents, lower.thresh=seq(from = 1, to = 50, by = 1))
```

```{r fit stm, cache=TRUE}
#structure and index for usage in the stm model. Verify no-missingness.
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 3)

#output will have object meta, documents, and vocab
docs <- out$documents
vocab <- out$vocab
meta  <-out$meta
##############

# released and fromH as a covariate in the topic prevalence
t1 <- Sys.time()
emailsFit <- stm(out$documents,
                 out$vocab,
                 K=40,
                 prevalence = ~ sentiment + from_to + released + day_week + month + year, 
                 max.em.its = 500,
                 data=out$meta,
                 seed=5926696,
                 verbose=FALSE,
                 init.type="Spectral")
t2 <- Sys.time()
t2-t1 # 16.81741 mins (converge around iteration 160)
# beepr::beep(0)

saveRDS(out, file = "out.RDS")
saveRDS(emailsFit, file = "emailsFit.RDS")
```

```{r stmBrowser, eval=FALSE}
out <- readRDS("out.RDS")
emailsFit <- readRDS("emailsFit")

# stmBrowser
stmBrowser(mod = emailsFit, 
           data = out$meta, 
           covariates = c("sentiment", "from_to", "released", "day_week", "month", "year"),
           text = "emails",
           id = NULL,
           n = 7000, 
           labeltype ="frex") #prob
```

[
```{r, stm image, fig.width=8, echo=FALSE}
img2 <- readPNG("images/stm.png")
grid.raster(img2)
```
](http://fredus14.github.io/Clinton_Email_Browser/)

Visualization of the Structural Topic Model created with the stmBrowser package. Click on this [link](http://fredus14.github.io/Clinton_Email_Browser/) or on the figure above to open the analysis in a web browser for interactive visualization.


## Session information

```{r session_info}
devtools::session_info()
```

This document was processed on: `r Sys.Date()`.