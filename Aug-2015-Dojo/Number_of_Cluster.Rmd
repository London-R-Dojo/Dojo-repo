---
title: "How to choose the optimal number of cluster"
output: html_document
---

<h2> Introduction </h2>

At the last coding dojo, the interrogation we get was the following:
Is it possible to create a function which automatically define the optimal number of cluster?
As usual, the answer with R is: there is a package to answer this question.

<h2> Training data set </h2>

First, we generate some fake data:
Not too much separated, but not too messy. It is a simulation, not real life :)

```{r, echo = F}
library(ggplot2)

sd <- 30

mat = data.frame(x =c(seq(1,10),seq(100,120),seq(10:30)) + rnorm(52, 0, sd), 
                 y = c(seq(1,10),seq(100,120),seq(300,320))+ rnorm(52, 0, sd)
                 , c = c(rep("a", 10), rep("b", 21), rep("c", 21)))

ggplot(data = mat, aes(x = x, y = y)) +
    geom_point( size = 4) 

```


<h2> Analysis </h2>

<<<<<<< HEAD
Our main inspiration is that post on stackoverflow:

http://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters

And the help of the Nbclust package.

=======
>>>>>>> parent of 1f1860b... Revert "number of cluster"
```{r, echo=FALSE}
library(NbClust)
```

The first way to determine a reasonnable number of cluster that I learn at school was the elbow plot.
The concept is to plot the sum of the distance between the centroid of the cluster and the point of the cluster by cluster. 

The plot looks like an elbow and the classic rule is to take the number of cluster where the curve begin to flaten. Afterward, each new cluster doesn't create a really new separated cluster.

Elbow plot:

```{r}
wss <- (nrow(mat)-1)*sum(apply(mat[, -3],2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(mat[, -3],
                                       centers=i)$withinss)

wss2 <- data.frame(x = 1:15, wss = wss)

ggplot(data = wss2, aes(x = x, y = wss))+
    geom_point(size = 4) +
  geom_line() +
  scale_x_continuous(breaks = 1:15) +
  ggtitle("Elbow plot")

```

The function NBClust:

This function test a consequent set of method to determine the optimal number of clusters.

```{r, eval = F}
res <- NbClust(mat[, -3], diss=NULL, distance = "euclidean", min.nc=2, max.nc=6, 
             method = "kmeans", index = "all")

```

```{r, echo = F, message= F, include = F}
res <- NbClust(mat[, -3], diss=NULL, distance = "euclidean", min.nc=2, max.nc=6, 
             method = "kmeans", index = "all")

```

The different method used (minus the graphical ones) and the number of clusters picked by each:

```{r}
res$Best.nc
```

Most common value:(Without 0)

```{r}
summary(res$Best.nc[1,][res$Best.nc[1,]!= 0])
```

In the end, the median of all this method is choose. In this case, `r length(unique(res$Best.partition))`.

<h2> Result </h2>

The plot:

```{r}

mat$res <- res$Best.partition

ggplot(data = mat, aes(x = x, y = y, colour = factor(res))) +
    geom_point( size = 4) 
  
```


There is another approach we didn't had time to look, but which seems promising:
The package BHC which does bayesian hierarchical clustering could also provide us an insight on the best cluster.
